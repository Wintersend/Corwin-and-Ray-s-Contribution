---
title: "Bayesian Autoregressions"
author:
  - name: "Tomasz WoÅºniak"
    affiliation: University of Melbourne
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378
  - name: "Your Name"
execute:
  
  echo: false
citation: 
  issued: 2023-05-25
  url: https://donotdespair.github.io/Bayesian-Autoregressions/
  doi: 10.26188/23255657
bibliography: references.bib
editor: 
  markdown: 
    wrap: 72
---

> **Abstract.** We present the basics of Bayesian estimation and
> inference for autoregressive models. The range of topics includes the
> natural conjugate analysis using normal-inverted-gamma 2 prior
> distribution and its extensions focusing on hierarchical modelling,
> conditional heteroskedasticity, and Student-t error terms. We focus on
> forecasting and sampling from the predictive density.
>
> **Keywords.** Autoregressions, Bayesian Inference, Forecasting,
> Heteroskedasticity, Hierarchical Modelling, Natural Conjugacy,
> Shrinkage Prior

# Autoregressions

Autoregressions are a popular class of linear models that are the most
useful for time series persistence analysis and forecasting a random
variable's unknown future values. The simplicity of their formulation,
estimation, and range of applications in which they occur useful decides
on their continued employment.

## The AR($p$) model

The model is set for a univariate time series whose observation at time
$t$ is denoted by $y_t$. It includes a $d$-vector $d_t$ of deterministic
terms and $p$ lags of the dependent variable on the right-hand side of
the model equation. It is complemented by error term $u_t$ that, in this
note, is zero-mean normally distributed with variance $\sigma^2$. Then
the model equations are: \begin{align}
y_t &= \alpha_d' d_t + \alpha_1 y_{t-1} + \dots + \alpha_p y_{t-p} + u_t\\
u_t\mid d_t, y_{t-1}, \dots, y_{t-p} &\sim\mathcal{N}\left(0, \sigma^2\right)
\end{align} where $\alpha_d$ is a $d$-vector of coefficients on
deterministic terms, and parameters $\alpha_1,\dots,\alpha_p$ are
autoregressive slopes.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation
for the model. Let $T$ be the available sample size for the variable
$y$. Define a $T$-vector of zeros, $\mathbf{0}_T$, the identity matrix
of order $T$, $\mathbf{I}_T$, $T\times1$ vectors: \begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots \\ y_T\end{bmatrix}, \quad
\text{ and }\quad
\mathbf{u} = \begin{bmatrix} u_1\\ \vdots \\ u_T\end{bmatrix},
\end{align} a $k\times1$ vector
$\mathbf{x}_t = \begin{bmatrix}d_t' & y_{t-1}&\dots& y_{t-} \end{bmatrix}'$,
where $k=d+p$, and a $T\times k$ matrix collecting the explanatory
variables: \begin{align}
\mathbf{X} = \begin{bmatrix} \mathbf{x}_1'\\ \vdots \\ \mathbf{x}_T'\end{bmatrix}.
\end{align} Collect the parameters of the conditional mean equation in a
$k$-vector: \begin{align}
\boldsymbol\alpha = \begin{bmatrix} \alpha_d'& \alpha_1 & \dots & \alpha_p\end{bmatrix}'.
\end{align}

Then the model can be written in a concise notation as: \begin{align}
\mathbf{y} &= \mathbf{X} \boldsymbol\alpha + \mathbf{u}\\
\mathbf{u}\mid \mathbf{X} &\sim\mathcal{N}_T\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector
$\mathbf{y}$. To see this, consider the model equation as a linear
transformation of a normal vector $\mathbf{u}$. Therefore, the data
vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \mathbf{X}, \boldsymbol\alpha, \sigma^2 &\sim\mathcal{N}_T\left(\mathbf{X} \boldsymbol\alpha, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that
is defined as the sampling data density: \begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X})\equiv p\left(\mathbf{y}\mid \mathbf{X}, \boldsymbol\alpha, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the
parameters, and after plugging in data in place of matrices $\mathbf{y}$
and $\mathbf{X}$, is considered a function of parameters
$\boldsymbol\alpha$ and $\sigma^2$ is given by: \begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)'(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)\right\}.
\end{align}

# Natural-Conjugate Analysis

## Normal-inverted gamma 2 prior

## Normal-inverted gamma 2 posterior

## Sampling draws from the posterior

# Hierarchical Prior Analysis

## Estimating autoregressive prior shrinkage

### Inverted gamma 2 scale mixture of normal

### Gamma scale mixture of normal

## Estimating error term variance prior scale

## Dummy observation prior

# Model Extensions

## Student-$t$ error term

## Estimating autoregressions after 2020

The 2020 COVID-19 pandemic significantly altered the global economic
landscape. It may be argued that the pre and post COVID periods are not
easily comparable, or that the most severe changes for COVID
would best be discounted due to their warping effect on the overall
data. However, @lenza2022estimate disagree. In their paper *How to estimate a vector autoregression after March 2020*, rather
than discount data, they suggest that the effects of COVID be modeled as
a temporary spike in volatility. They found that the first three periods
of the pandemic are where volatility is the most unpredictable and this
finding holds regardless of whether monthly or quarterly data is being
used. For higher frequency data the exact effect is unknown though should cover at least the first 3 months of the pandemic. Thus, they propose the following change to the standard formula for autoregressions:

$$y_t = \mathbf{x}_t'\boldsymbol\alpha + c_tu_t,$$

where $c_t$ is a standard deviation multiplier. That is, for every
period before COVID it takes a value of 1, for the first 3 periods of
COVID it takes values $\bar{c}_0$, $\bar{c}_1$, $\bar{c}_2$, and then in all future periods a value of 
$$c_{t*+j} = 1 + (\bar{c}_2 - 1)\rho^{j-2}, $$
where $\rho\in(0,1)$ captures the exponential decay in the value of the conditional standard deviation towards the value one. This creates a vector that leaves the error term unchanged before COVID, has a surge in volatility during COVID, and then decays geometrically after COVID. This structure approximates the observed shocks to volatility and facilitates straightforward estimation.

By dividing both sides by $c_t$ the model equation can be rewritten as 
$$\bar{y}_t = \bar{\mathbf{x}}_t'\boldsymbol\alpha + u_t$$
where $\bar{y}_t = y_t/c_t$ and $\bar{\mathbf{x}}_t = \mathbf{x}_t/c_t$. These simple transformations then lend themselves to analysis using whatever estimation method is preferred.

The main difficulty arises in the estimation of $c_t$ as it is an unknown parameter in many cases.

### Methodology

To estimate the values for $c_0$, $c_1$, and $c_2$ define a $T$-vector $\mathbf{c}$ of COVID volatility variables:

$$\mathbf{c}=\begin{bmatrix}1&\dots& c_0 & c_1 & c_2 & 1+(c_2-1)\rho &  1+(c_2-1)\rho^2&\dots\end{bmatrix}'$$

Intuitively, volatility variable for the period before COVID is set to unity. Heightened volatility during the first three quarters of COVID are parameterized, then they are assumed to decay at a geometric rate beginning at the fourth quarter since the pandemic's onset.

These COVID volatility parameters are collected in a vector $\boldsymbol\theta=(c_0\quad c_1\quad c_2\quad\rho)$ and are estimated from their own marginal posterior as proposed by @lenza2022estimate:

$$p(\boldsymbol\theta\mid\mathbf{y},\mathbf{X},\underline{\gamma})\propto P(\mathbf{y}\mid\mathbf{X},\boldsymbol\theta,\underline{\gamma})p(\boldsymbol\theta\mid\underline{\gamma})$$

where the likelihood function is given as:

$$P(Y,X|\theta,\underline{\gamma})\propto \Bigg(\prod^T_{t=1}c_t^{-N}\Bigg)||\underline{V}||^{\frac{N}{2}}||\underline{S}||^{\frac{\underline{\nu}}{2}}||\tilde{X}'\tilde{X}+\underline{V}^{-1}||^{\frac{N}{2}}\\$$

$$||\underline{S}+\hat{\tilde{U}}'\hat{\tilde{U}}+(\hat{\tilde{A}}-\tilde{Y}+\tilde{X}\underline{A})'\underline{V}^{-1}(\hat{\tilde{A}}-\tilde{Y}+\tilde{X}\underline{A})||^{-\frac{T-p+\underline{\nu}}{2}}$$

where $\tilde{X}_t=\frac{(1,Y_t,...,Y_{t-p})'}{c_t}$, $\tilde{\hat{A}}=(\tilde{X}'\tilde{X}-\underline{V}^{-1})^{-1}$, and $\underline{\gamma}=(\underline{A},\underline{S},\underline{V},\underline{\nu})$

and the priors are assumed to be: $c_0,c_1,c_2\sim Pareto(1,1)$ and $\rho\sim Beta(3,1.5)$


## Stochastic volatility heteroskedasticity

# Forecasting

## Conditional predictive density

## Algorithm to sample from the predictive density

## Sampler implementation in R

# References {.unnumbered}
