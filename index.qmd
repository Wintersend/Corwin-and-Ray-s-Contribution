---
title: "Bayesian Autoregressions"
author:
  - name: "Tomasz WoÅºniak"
    affiliation: University of Melbourne
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378
  - name: "Your Name"

execute:
  echo: false
  
citation: 
  issued: 2023-05-25
  url: https://donotdespair.github.io/Bayesian-Autoregressions/
  doi: 10.26188/23255657
bibliography: references.bib
---

> **Abstract.** We present the basics of Bayesian estimation and inference for autoregressive models. The range of topics includes the natural conjugate analysis using normal-inverted-gamma 2 prior distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We focus on forecasting and sampling from the predictive density.
>
> **Keywords.** Autoregressions, Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Natural Conjugacy, Shrinkage Prior

# Autoregressions

Autoregressions are a popular class of linear models that are the most useful for time series persistence analysis and forecasting a random variable's unknown future values. The simplicity of their formulation, estimation, and range of applications in which they occur useful decides on their continued employment. 

## The AR($p$) model

The model is set for a univariate time series whose observation at time $t$ is denoted by $y_t$. It includes a $d$-vector $d_t$ of deterministic terms and $p$ lags of the dependent variable on the right-hand side of the model equation. It is complemented by error term $u_t$ that, in this note, is zero-mean normally distributed with variance $\sigma^2$. Then the model equations are:
\begin{align}
y_t &= \alpha_d' d_t + \alpha_1 y_{t-1} + \dots + \alpha_p y_{t-p} + u_t\\
u_t\mid d_t, y_{t-1}, \dots, y_{t-p} &\sim\mathcal{N}\left(0, \sigma^2\right)
\end{align}
where $\alpha_d$ is a $d$-vector of coefficients on deterministic terms, and parameters $\alpha_1,\dots,\alpha_p$ are autoregressive slopes.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation for the model. Let $T$ be the available sample size for the variable $y$. Define a $T$-vector of zeros, $\mathbf{0}_T$, the identity matrix of order $T$, $\mathbf{I}_T$, $T\times1$ vectors:
\begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots \\ y_T\end{bmatrix}, \quad
\text{ and }\quad
\mathbf{u} = \begin{bmatrix} u_1\\ \vdots \\ u_T\end{bmatrix},
\end{align}
a $k\times1$ vector $\mathbf{x}_t = \begin{bmatrix}d_t' & y_{t-1}&\dots& y_{t-} \end{bmatrix}'$, where $k=d+p$, and a $T\times k$ matrix collecting the explanatory variables:
\begin{align}
\mathbf{X} = \begin{bmatrix} \mathbf{x}_1'\\ \vdots \\ \mathbf{x}_T'\end{bmatrix}.
\end{align}
Collect the parameters of the conditional mean equation in a $k$-vector:
\begin{align}
\boldsymbol\alpha = \begin{bmatrix} \alpha_d'& \alpha_1 & \dots & \alpha_p\end{bmatrix}'.
\end{align}

Then the model can be written in a concise notation as:
\begin{align}
\mathbf{y} &= \mathbf{X} \boldsymbol\alpha + \mathbf{u}\\
\mathbf{u}\mid \mathbf{X} &\sim\mathcal{N}_T\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector $\mathbf{y}$. To see this, consider the model equation as a linear transformation of a normal vector $\mathbf{u}$. Therefore, the data vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \mathbf{X}, \boldsymbol\alpha, \sigma^2 &\sim\mathcal{N}_T\left(\mathbf{X} \boldsymbol\alpha, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that is defined as the sampling data density:
\begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X})\equiv p\left(\mathbf{y}\mid \mathbf{X}, \boldsymbol\alpha, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of matrices $\mathbf{y}$ and $\mathbf{X}$, is considered a function of parameters $\boldsymbol\alpha$ and $\sigma^2$ is given by:
\begin{align}
L(\boldsymbol\alpha,\sigma^2|\mathbf{y}, \mathbf{X}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)'(\mathbf{y} - \mathbf{X}\boldsymbol\alpha)\right\}.
\end{align}


# Natural-Conjugate Analysis

## Normal-inverted gamma 2 prior

## Normal-inverted gamma 2 posterior

## Sampling draws from the posterior



# Hierarchical Prior Analysis

## Estimating autoregressive prior shrinkage

### Inverted gamma 2 scale mixture of normal

### Gamma scale mixture of normal

## Estimating error term variance prior scale

## Dummy observation prior




# Model Extensions

## Student-$t$ error term

## Estimating autoregressions after 2020

## Stochastic volatility heteroskedasticity



# Forecasting

## Conditional predictive density

The Bayesian approach of the conditional predictive density is a combination of a predictive density from the frequentist approach and the posterior distribution of the unknown parameters $\alpha$ and $\sigma^{2}$. The one-period ahead conditional predictive density can be expressed as below. 

\begin{align}
p(y_{t+1} |x_{t+1}, Y, X) &= \int\int p(y_{t+1} |x_{t+1}, Y, X, {\color{Red} \alpha}, {\color{Red}\sigma^{2}})p({\color{Red} \alpha}, {\color{Red}\sigma^{2}}|Y,X)d{\color{Red} \alpha} d{\color{Red}\sigma^{2}}\\\\
p(y_{t+1} |x_{t+1}, Y, X, {\color{Red} \alpha}, {\color{Red}\sigma^{2}}) &= N_{T}( {\color{Red}\alpha'_{d}}d_{t+1|t} + {\color{Red}\alpha_{1}}y_{t}+...+{\color{Red}\alpha_{p}}y_{t-p+1}, {\color{Red}\sigma^{2}}I_{T})\\
p({\color{Red} \alpha}, {\color{Red}\sigma^{2}}|Y,X) &= NIG2(\overline{\alpha}, \overline{\sigma^{2}}, \overline{S}, \overline{v}) \\
\end{align}

To sample the joint predictive density, we integrate out $\alpha$ and $\sigma^{2}$. Below steps show how the one-period ahead joint predictive density can be obtained. 

Sample draws from the posterior distribution $p({\color{Red} \alpha}, {\color{Red}\sigma^{2}}|Y,X)$ and 

Obtain $\left\{ \alpha^{(s)},  \sigma^{2(s)} \right\}^{S}_{s=1}$

Sample draws from $\widehat{p}(y_{t+1} |x_{t+1})$ by $y_{t+1} \sim N (y_{t+1|t}(\alpha^{(s)}), Var[y_{t+1|t}|\alpha^{(s)},  \sigma^{2(s)}])$

Obtain $\left\{ y_{t+1|t}^{(s)} \right\}^{S}_{s=1}$

Characterise the predictive density using $\left\{ y_{t+1|t}^{(s)} \right\}^{S}_{s=1}$


## Algorithm to sample from the predictive density

## Sampler implementation in R

```{r forecasting h step}
#| echo: true
#| message: false
#| warning: false
#| eval: false
## Apply function when needed
posterior.sample.draws = posterior.draws(S=50000, Y=Y, X=X)         # using the posterior function to draw alpha and sigma
A.posterior.simu       = posterior.sample.draws$A.posterior         # obtain posterior alpha
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior     # obtain posterior sigma

## forecasting setup
h                      = 12                                         # specify the desired number of steps ahead
S                      = 50000                                      # the number of sampling time, no greater than the simulation time in the posterior draw function
Y.h                    = matrix(NA,h,S)                             # create empty matrix to store the h-step ahead Y 

## sampling predictive density
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,s]
    x.Ti               = Y[(nrow(Y)-p+1):nrow(Y)]                   # the number of lags in Y
    x.Ti               = x.Ti[p:1]
  for (i in 1:h){
    x.T                = c(d,as.vector(t(x.Ti)))                    # d refers to d row vector for the deterministic term data
    Y.f                = rnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
      x.Ti             = rbind(Y.f,x.Ti[1:(p-1)])                   # refresh the initial data in Y
    Y.h[i,s]           = Y.f
  }
}
```











# References {.unnumbered}