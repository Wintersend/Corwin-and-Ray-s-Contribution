[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Autoregressions",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for autoregressive models. The range of topics includes the natural conjugate analysis using normal-inverted-gamma 2 prior distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We focus on forecasting and sampling from the predictive density.\nKeywords. Autoregressions, Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Natural Conjugacy, Shrinkage Prior"
  },
  {
    "objectID": "index.html#the-arp-model",
    "href": "index.html#the-arp-model",
    "title": "Bayesian Autoregressions",
    "section": "The AR(\\(p\\)) model",
    "text": "The AR(\\(p\\)) model\nThe model is set for a univariate time series whose observation at time \\(t\\) is denoted by \\(y_t\\). It includes a \\(d\\)-vector \\(d_t\\) of deterministic terms and \\(p\\) lags of the dependent variable on the right-hand side of the model equation. It is complemented by error term \\(u_t\\) that, in this note, is zero-mean normally distributed with variance \\(\\sigma^2\\). Then the model equations are: \\[\\begin{align}\ny_t &= \\alpha_d' d_t + \\alpha_1 y_{t-1} + \\dots + \\alpha_p y_{t-p} + u_t\\\\\nu_t\\mid d_t, y_{t-1}, \\dots, y_{t-p} &\\sim\\mathcal{N}\\left(0, \\sigma^2\\right)\n\\end{align}\\] where \\(\\alpha_d\\) is a \\(d\\)-vector of coefficients on deterministic terms, and parameters \\(\\alpha_1,\\dots,\\alpha_p\\) are autoregressive slopes."
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Autoregressions",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), \\(T\\times1\\) vectors: \\[\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots \\\\ y_T\\end{bmatrix}, \\quad\n\\text{ and }\\quad\n\\mathbf{u} = \\begin{bmatrix} u_1\\\\ \\vdots \\\\ u_T\\end{bmatrix},\n\\end{align}\\] a \\(k\\times1\\) vector \\(\\mathbf{x}_t = \\begin{bmatrix}d_t' & y_{t-1}&\\dots& y_{t-} \\end{bmatrix}'\\), where \\(k=d+p\\), and a \\(T\\times k\\) matrix collecting the explanatory variables: \\[\\begin{align}\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1'\\\\ \\vdots \\\\ \\mathbf{x}_T'\\end{bmatrix}.\n\\end{align}\\] Collect the parameters of the conditional mean equation in a \\(k\\)-vector: \\[\\begin{align}\n\\boldsymbol\\alpha = \\begin{bmatrix} \\alpha_d'& \\alpha_1 & \\dots & \\alpha_p\\end{bmatrix}'.\n\\end{align}\\]\nThen the model can be written in a concise notation as: \\[\\begin{align}\n\\mathbf{y} &= \\mathbf{X} \\boldsymbol\\alpha + \\mathbf{u}\\\\\n\\mathbf{u}\\mid \\mathbf{X} &\\sim\\mathcal{N}_T\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Autoregressions",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\mathbf{u}\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\\begin{align}\n\\mathbf{y}\\mid \\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\mathbf{X} \\boldsymbol\\alpha, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\\begin{align}\nL(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y}, \\mathbf{X})\\equiv p\\left(\\mathbf{y}\\mid \\mathbf{X}, \\boldsymbol\\alpha, \\sigma^2 \\right).\n\\end{align}\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of matrices \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\), is considered a function of parameters \\(\\boldsymbol\\alpha\\) and \\(\\sigma^2\\) is given by: \\[\\begin{align}\nL(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y}, \\mathbf{X}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\alpha)'(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\alpha)\\right\\}.\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#normal-inverted-gamma-2-prior",
    "href": "index.html#normal-inverted-gamma-2-prior",
    "title": "Bayesian Autoregressions",
    "section": "Normal-inverted gamma 2 prior",
    "text": "Normal-inverted gamma 2 prior"
  },
  {
    "objectID": "index.html#normal-inverted-gamma-2-posterior",
    "href": "index.html#normal-inverted-gamma-2-posterior",
    "title": "Bayesian Autoregressions",
    "section": "Normal-inverted gamma 2 posterior",
    "text": "Normal-inverted gamma 2 posterior"
  },
  {
    "objectID": "index.html#sampling-draws-from-the-posterior",
    "href": "index.html#sampling-draws-from-the-posterior",
    "title": "Bayesian Autoregressions",
    "section": "Sampling draws from the posterior",
    "text": "Sampling draws from the posterior"
  },
  {
    "objectID": "index.html#estimating-autoregressive-prior-shrinkage",
    "href": "index.html#estimating-autoregressive-prior-shrinkage",
    "title": "Bayesian Autoregressions",
    "section": "Estimating autoregressive prior shrinkage",
    "text": "Estimating autoregressive prior shrinkage\n\nInverted gamma 2 scale mixture of normal\n\n\nGamma scale mixture of normal"
  },
  {
    "objectID": "index.html#estimating-error-term-variance-prior-scale",
    "href": "index.html#estimating-error-term-variance-prior-scale",
    "title": "Bayesian Autoregressions",
    "section": "Estimating error term variance prior scale",
    "text": "Estimating error term variance prior scale"
  },
  {
    "objectID": "index.html#dummy-observation-prior",
    "href": "index.html#dummy-observation-prior",
    "title": "Bayesian Autoregressions",
    "section": "Dummy observation prior",
    "text": "Dummy observation prior"
  },
  {
    "objectID": "index.html#student-t-error-term",
    "href": "index.html#student-t-error-term",
    "title": "Bayesian Autoregressions",
    "section": "Student-\\(t\\) error term",
    "text": "Student-\\(t\\) error term"
  },
  {
    "objectID": "index.html#estimating-autoregressions-after-2020",
    "href": "index.html#estimating-autoregressions-after-2020",
    "title": "Bayesian Autoregressions",
    "section": "Estimating autoregressions after 2020",
    "text": "Estimating autoregressions after 2020\nThe 2020 COVID-19 pandemic significantly altered the global economic landscape. It may be argued that the pre and post COVID periods are not easily comparable, or that the most severe changes for COVID would best be discounted due to their warping effect on the overall data. However, Lenza and Primiceri (2022) disagree. In their paper How to estimate a vector autoregression after March 2020, rather than discount data, they suggest that the effects of COVID be modeled as a temporary spike in volatility. They found that the first three periods of the pandemic are where volatility is the most unpredictable and this finding holds regardless of whether monthly or quarterly data is being used. For higher frequency data the exact effect is unknown though should cover at least the first 3 months of the pandemic. Thus, they propose the following change to the standard formula for autoregressions:\n\\[y_t = \\mathbf{x}_t'\\boldsymbol\\alpha + c_tu_t,\\]\nwhere \\(c_t\\) is a standard deviation multiplier. That is, for every period before COVID it takes a value of 1, for the first 3 periods of COVID it takes values \\(\\bar{c}_0\\), \\(\\bar{c}_1\\), \\(\\bar{c}_2\\), and then in all future periods a value of \\[c_{t*+j} = 1 + (\\bar{c}_2 - 1)\\rho^{j-2}, \\] where \\(\\rho\\in(0,1)\\) captures the exponential decay in the value of the conditional standard deviation towards the value one. This creates a vector that leaves the error term unchanged before COVID, has a surge in volatility during COVID, and then decays geometrically after COVID. This structure approximates the observed shocks to volatility and facilitates straightforward estimation.\nBy dividing both sides by \\(c_t\\) the model equation can be rewritten as \\[\\bar{y}_t = \\bar{\\mathbf{x}}_t'\\boldsymbol\\alpha + u_t\\] where \\(\\bar{y}_t = y_t/c_t\\) and \\(\\bar{\\mathbf{x}}_t = \\mathbf{x}_t/c_t\\). These simple transformations then lend themselves to analysis using whatever estimation method is preferred.\nThe main difficulty arises in the estimation of \\(c_t\\) as it is an unknown parameter in many cases.\n\nMethodology\nTo estimate the values for \\(c_0\\), \\(c_1\\), and \\(c_2\\) define a \\(T\\)-vector \\(\\mathbf{c}\\) of COVID volatility variables:\n\\[\\mathbf{c}=\\begin{bmatrix}1&\\dots& c_0 & c_1 & c_2 & 1+(c_2-1)\\rho &  1+(c_2-1)\\rho^2&\\dots\\end{bmatrix}'\\]\nIntuitively, volatility variable for the period before COVID is set to unity. Heightened volatility during the first three quarters of COVID are parameterized, then they are assumed to decay at a geometric rate beginning at the fourth quarter since the pandemicâ€™s onset.\nThese COVID volatility parameters are collected in a vector \\(\\boldsymbol\\theta=(c_0\\quad c_1\\quad c_2\\quad\\rho)\\) and are estimated from their own marginal posterior as proposed by Lenza and Primiceri (2022):\n\\[p(\\boldsymbol\\theta\\mid\\mathbf{y},\\mathbf{X},\\underline{\\gamma})\\propto P(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol\\theta,\\underline{\\gamma})p(\\boldsymbol\\theta\\mid\\underline{\\gamma})\\]\nwhere the likelihood function is given as:\n\\[P(\\mathbf{y},\\mathbf{X}|\\boldsymbol\\theta,\\underline{\\gamma})\\propto \\Bigg(\\prod^T_{t=1}c_t^{-N}\\Bigg)||\\underline{V}||^{\\frac{N}{2}}||\\underline{S}||^{\\frac{\\underline{\\nu}}{2}}||\\tilde{X}'\\tilde{X}+\\underline{V}^{-1}||^{\\frac{N}{2}}\\\\\\]\n\\[||\\underline{S}+\\hat{\\tilde{\\mathbf{u}}}'\\hat{\\tilde{\\mathbf{u}}}+(\\hat{\\tilde{\\boldsymbol\\alpha}}-\\tilde{Y}+\\tilde{\\mathbf{X}}\\underline{\\boldsymbol\\alpha})'\\underline{V}^{-1}(\\hat{\\tilde{\\boldsymbol\\alpha}}-\\tilde{\\mathbf{y}}+\\tilde{\\mathbf{X}}\\underline{\\boldsymbol\\alpha})||^{-\\frac{T-p+\\underline{\\nu}}{2}}\\] Then, after substituting in data for y and X, the likelihood function for estimation of the parameters is given by:\n\\[L(\\boldsymbol\\alpha,\\sigma^2|\\mathbf{y}, \\mathbf{X}) = (2\\pi)^{-\\frac{T}{2}}\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\alpha)'(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\alpha)\\right\\}\\]\nwhere \\(\\tilde{\\mathbf{x}}_t=\\frac{(1,\\mathbf{y}_t,...,\\mathbf{y}_{t-p})'}{\\mathbf{c}_t}\\), \\(\\tilde{\\hat{A}}=(\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}-\\underline{V}^{-1})^{-1}\\), and \\(\\underline{\\gamma}=(\\underline{A},\\underline{S},\\underline{V},\\underline{\\nu})\\)\nand the priors are assumed to be: \\(\\mathbf{c}_0,\\mathbf{c}_1,\\mathbf{c}_2\\sim Pareto(1,1)\\) and \\(\\rho\\sim Beta(3,1.5)\\)"
  },
  {
    "objectID": "index.html#stochastic-volatility-heteroskedasticity",
    "href": "index.html#stochastic-volatility-heteroskedasticity",
    "title": "Bayesian Autoregressions",
    "section": "Stochastic volatility heteroskedasticity",
    "text": "Stochastic volatility heteroskedasticity"
  },
  {
    "objectID": "index.html#conditional-predictive-density",
    "href": "index.html#conditional-predictive-density",
    "title": "Bayesian Autoregressions",
    "section": "Conditional predictive density",
    "text": "Conditional predictive density"
  },
  {
    "objectID": "index.html#algorithm-to-sample-from-the-predictive-density",
    "href": "index.html#algorithm-to-sample-from-the-predictive-density",
    "title": "Bayesian Autoregressions",
    "section": "Algorithm to sample from the predictive density",
    "text": "Algorithm to sample from the predictive density"
  },
  {
    "objectID": "index.html#sampler-implementation-in-r",
    "href": "index.html#sampler-implementation-in-r",
    "title": "Bayesian Autoregressions",
    "section": "Sampler implementation in R",
    "text": "Sampler implementation in R"
  }
]